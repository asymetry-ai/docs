---
title: "Security Analysis"
description: "How Asymetry analyzes LLM interactions for security risks"
sidebarTitle: "Security Analysis"
---

# Security Analysis

Asymetry automatically evaluates LLM interactions for security risks, including prompt injection, jailbreak attempts, PII exposure, and toxic content. This analysis happens asynchronously for all traces ingested via the SDK.

---

## Analysis Results

Traces in your dashboard will be annotated with the following scores and flags:

### Risk Scores

| Score Type | Range | Description |
| --- | --- | --- |
| **Prompt Risk Score** | 0-100 | How risky the user's input is (0 = safe, 100 = high risk) |
| **Output Safety Score** | 0-100 | How risky the model's output is (0 = safe, 100 = high risk) |

### PII Flags

| Flag | Description |
| --- | --- |
| `email_detected` | Email addresses found |
| `phone_detected` | Phone numbers found |
| `credit_card_detected` | Credit card numbers found |
| `ssn_detected` | Social Security Numbers found |
| `api_key_detected` | API keys or secrets found |
| `other_sensitive_detected` | Other sensitive information |
| `entity_counts` | Count of each PII entity type |

### Security Flags (Input)

| Flag | Description |
| --- | --- |
| `prompt_injection` | Attempt to override system instructions |
| `jailbreak_attempt` | Attempt to bypass safety measures |
| `policy_evasion` | Try to evade content policies |
| `malicious_intent` | Harmful or malicious content |
| `unknown_risk` | Unclassified risk detected |
| `successful_defense` | Model refused the risky request |

### Output Flags

| Flag | Description |
| --- | --- |
| `violence_hate` | Violent or hateful content |
| `malware_exploit` | Malware or exploit information |
| `toxicity` | Toxic or harmful language |
| `hallucinated_secrets` | AI generated fake credentials |
| `policy_violation` | Content violating policies |

---

## Detailed Findings

Each analysis may generate specific "Findings" which provide more context:

| Field | Description |
| --- | --- |
| `category` | `Prompt Risk`, `Output Safety`, or `PII` |
| `type` | Specific risk type (e.g. "Prompt Injection") |
| `score` | Confidence score (0-100) |
| `source` | Detection method used |
| `snippet` | Text that triggered the finding |

---

## Score Interpretation

### Prompt Risk Score

| Range | Interpretation | Action |
| --- | --- | --- |
| 0-30 | Low risk | Normal processing |
| 31-50 | Moderate risk | Log and monitor |
| 51-75 | High risk | Alert and review |
| 76-100 | Critical risk | Block or flag for human review |

### Output Safety Score

| Range | Interpretation | Action |
| --- | --- | --- |
| 0-30 | Safe output | Normal response |
| 31-50 | Questionable | Review for appropriateness |
| 51-75 | Unsafe | Consider filtering |
| 76-100 | Dangerous | Block from user |

---

## Detection Capabilities

### PII Detection

Detects personally identifiable information in both input and output:

- **Email Addresses**: Standard email patterns
- **Phone Numbers**: 10-digit phone numbers
- **Credit Card Numbers**: Major credit card formats (13-16 digits)
- **SSN**: Social Security Numbers (if enabled)
- **Custom Entities**: Configurable pattern matching

### Secret Detection

Automatically identifies exposed secrets and API keys:

| Secret Type | Pattern |
| --- | --- |
| **AWS Access Key** | `AKIA...` (16 chars) |
| **GitHub Token** | `ghp_...` (36 chars) |
| **JWT** | JSON Web Tokens (`eyJ...`) |

### Injection Detection Logic

Uses a multi-signal heuristic approach to score prompt risk:

1.  **Intent Override**: Direct commands to ignore instructions or bypass policies (e.g., "Ignore previous instructions").
2.  **Hierarchy Violation**: Attempts to impersonate system roles or override the "system" prompt authority.
3.  **Structural Anomaly**: Excessive use of delimiters, markdown injection, or unconventional formatting designed to confuse the model.

Risk is calculated as a weighted score of these signals:
`Risk = 0.45 * Hierarchy + 0.35 * Intent + 0.2 * Structural`

---

## Usage Limits

| Plan | Analysis Runs/Month |
| --- | --- |
| Free | 1,000 |
| Pro | Unlimited (billed per 1,000) |

---

## Best Practices

<AccordionGroup>
  <Accordion title="Review automatically">
    Security analysis runs automatically for SDK-ingested traces. Check your dashboard periodically to review flagged traces.
  </Accordion>

  <Accordion title="Handle high-risk scores">
    For scores above 75, consider:
    - Flagging for human review
    - Blocking the response (if checking in real-time)
    - Logging for security audits
  </Accordion>

  <Accordion title="Monitor successful_defense">
    When `successful_defense` is true, the model refused a risky request. This is good! Track these to understand attack patterns against your application.
  </Accordion>
</AccordionGroup>

---

## Next Steps

<CardGroup cols={2}>
  <Card title="View Dashboard" icon="chart-bar" href="https://asymetry.co">
    See security analysis in action
  </Card>
  <Card title="SDK Overview" icon="book" href="/sdk/overview">
    Learn about automatic analysis
  </Card>
</CardGroup>
